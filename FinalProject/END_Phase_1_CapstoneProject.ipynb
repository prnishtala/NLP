{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "END Phase 1_CapstoneProject",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYjSI5A1Aw/W1T/czpTHF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prnishtala/NLP/blob/main/FinalProject/END_Phase_1_CapstoneProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfBDTt4dJCRI"
      },
      "source": [
        "# Importing Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFS3F6-KIaXO",
        "outputId": "cc7b6665-a4fe-431e-80ab-98cecf404c0a"
      },
      "source": [
        "import re\n",
        "import spacy\n",
        "import torch\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.legacy.data import Field, BucketIterator, TabularDataset, Dataset, Example\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "import torch.nn.functional as F\n",
        "import keyword, token, tokenize\n",
        "import spacy\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.set_deterministic(True)\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/__init__.py:422: UserWarning: torch.set_deterministic is deprecated and will be removed in a future release. Please use torch.use_deterministic_algorithms instead\n",
            "  \"torch.set_deterministic is deprecated and will be removed in a future \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP5MKsU_IscR"
      },
      "source": [
        "# Data Cleansing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzJyubE4JH_N",
        "outputId": "76666491-6b3b-450e-d55d-300f6a79055b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! cp \"/content/drive/My Drive/NLP/english_python_data_raw.txt\" english_python_data.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn_hmqkDOkL7"
      },
      "source": [
        "# Read the file and remove the unwanted data\n",
        "# Removed few anomalies manually to prevent over-engineering\n",
        "\n",
        "with open('english_python_data.txt', 'r') as f:\n",
        "  raw_data = f.readlines()\n",
        "\n",
        "cleansed_data = []\n",
        "\n",
        "for sentence in raw_data:\n",
        "  # Remove the comments indicating beginning of Driver Codes\n",
        "  if re.search(r'^\\s*#\\s+driver', sentence.lower()):\n",
        "    continue\n",
        "  # Remove the unwanted empty lines\n",
        "  elif re.search(r'^\\s*\\n+\\s*$', sentence.lower()):\n",
        "    continue\n",
        "  # Remove the comments in the pattern of \"In[0-9]\"\n",
        "  elif re.search(r'^\\s*#\\s+in\\[[0-9]+\\]', sentence.lower()):\n",
        "    continue\n",
        "  # Remove the comments containing just the numbers\n",
        "  elif re.search(r'^\\s*#+\\s*[0-9]*\\s*\\n', sentence.lower()):\n",
        "    continue\n",
        "  else:\n",
        "    cleansed_data.append(sentence)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08XoaJ4EuOvX"
      },
      "source": [
        "# Generate DataFrame from the Input Data\n",
        "\n",
        "df = pd.DataFrame(columns = ['src', 'trg'])\n",
        "\n",
        "src = ''\n",
        "trg = ''\n",
        "minimum_length = 30\n",
        "\n",
        "for i,record in enumerate(cleansed_data):\n",
        "  if record.startswith('#') and len(record) > minimum_length:\n",
        "    df.loc[len(df)] = [src,trg]\n",
        "    trg = ''\n",
        "    src = record\n",
        "  else:\n",
        "    trg = trg + record\n",
        "\n",
        "df = df.iloc[1:]\n",
        "df = df.reset_index(drop=True)\n",
        " "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "249O1xm277t3",
        "outputId": "aedffe6e-55c5-4f47-a633-62c00f324571"
      },
      "source": [
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>trg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>write a python program to add two numbers</td>\n",
              "      <td>[num1 = 1.5, num2 = 6.3, sum = num1 + num2, pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>write a python function to add two user provi...</td>\n",
              "      <td>[def add_two_numbers(num1, num2):,     sum = n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>write a program to find and print the largest...</td>\n",
              "      <td>[num1 = 10, num2 = 12, num3 = 14, if (num1 &gt;= ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>write a program to find and print the smalles...</td>\n",
              "      <td>[num1 = 10, num2 = 12, num3 = 14, if (num1 &lt;= ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4990</th>\n",
              "      <td>write a program to print bit wise or of two n...</td>\n",
              "      <td>[a = 60, b = 13, c = a | b, print(\"or\", c), , ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4991</th>\n",
              "      <td>write a program to print bit wise xor of two ...</td>\n",
              "      <td>[a = 60, b = 13, c = a ^ b, print(\"xor\", c), , ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4992</th>\n",
              "      <td>write a program to calculate binary ones comp...</td>\n",
              "      <td>[a = 60, c = ~a, print(\"binary ones complement...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4993</th>\n",
              "      <td>write a program to binary left shift a number</td>\n",
              "      <td>[c = a &lt;&lt; 2, print(\"binary left shift\", c), , ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4994</th>\n",
              "      <td>write a program to binary right shift a number</td>\n",
              "      <td>[c = a &gt;&gt; 2, print(\"binary right shift\", c), ]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4995 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    src                                                trg\n",
              "0                                                                                                       []\n",
              "1             write a python program to add two numbers  [num1 = 1.5, num2 = 6.3, sum = num1 + num2, pr...\n",
              "2      write a python function to add two user provi...  [def add_two_numbers(num1, num2):,     sum = n...\n",
              "3      write a program to find and print the largest...  [num1 = 10, num2 = 12, num3 = 14, if (num1 >= ...\n",
              "4      write a program to find and print the smalles...  [num1 = 10, num2 = 12, num3 = 14, if (num1 <= ...\n",
              "...                                                 ...                                                ...\n",
              "4990   write a program to print bit wise or of two n...    [a = 60, b = 13, c = a | b, print(\"or\", c), , ]\n",
              "4991   write a program to print bit wise xor of two ...   [a = 60, b = 13, c = a ^ b, print(\"xor\", c), , ]\n",
              "4992   write a program to calculate binary ones comp...  [a = 60, c = ~a, print(\"binary ones complement...\n",
              "4993      write a program to binary left shift a number    [c = a << 2, print(\"binary left shift\", c), , ]\n",
              "4994     write a program to binary right shift a number     [c = a >> 2, print(\"binary right shift\", c), ]\n",
              "\n",
              "[4995 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdBCwGsI_mb-"
      },
      "source": [
        "# Tokenizing and Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2456KDRANWK"
      },
      "source": [
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXC-7vY0Cwsb"
      },
      "source": [
        "SRC = Field(tokenize = 'spacy', \n",
        "            eos_token = 'end',\n",
        "            init_token = '<sos>', \n",
        "            lower = True,\n",
        "            batch_first = True)\n",
        "\n",
        "TRG = Field(tokenize = 'spacy', \n",
        "            eos_token = 'end',\n",
        "            init_token = '<sos>', \n",
        "            lower = True,\n",
        "            batch_first = True)\n",
        "\n",
        "fields = [(\"src\", SRC), (\"trg\", TRG)]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yqoQHT1h2ux"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "fields = [(\"src\", SRC), (\"trg\", TRG)]\n",
        "\n",
        "examples = [data.Example.fromlist([df.src[i],df.trg[i]], fields) for i in range(df.shape[0])] \n",
        "\n",
        "# Preparing the Traning, Validation and Testing datasets\n",
        "dataset = data.Dataset(examples, fields)\n",
        "\n",
        "(train_data, valid_data, test_data) = dataset.split(split_ratio=[0.80, 0.10, 0.10], random_state=random.seed(SEED))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hl-MsEH5rkT"
      },
      "source": [
        "## Build Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjWtM0MjjC65"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 1)\n",
        "TRG.build_vocab(train_data, min_freq = 1)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNmHG9NOjQ62"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "     sort = False,\n",
        "     batch_size = BATCH_SIZE,\n",
        "     device = device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Un9TA5inDpt"
      },
      "source": [
        "spacy_en = spacy.load('en')\n",
        "def Tokenize(sentence):\n",
        "  sentence = str(sentence).replace('\\n', '\\t\\t')\n",
        "  return [tok.text for tok in spacy_en.tokenizer(sentence)]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc7B9gfW5vt8"
      },
      "source": [
        "## Gensim Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iQ46-NZTgXx",
        "outputId": "7bbe5d36-a259-4d1c-c103-6a2d7858fd65"
      },
      "source": [
        "# Using gensim for embedding Python syntaxes for Python code, gensim with Word2Vec model has be utilized and created a embedding with the same dimension ie 256.\n",
        "\n",
        "import gensim\n",
        "dim = 256\n",
        "min_count = 2\n",
        "window = 3\n",
        "target = []\n",
        "word2vec_vectors = []\n",
        "\n",
        "for rec in df['trg'].values:\n",
        "  rec_token = Tokenize(rec)\n",
        "  target.append(rec_token)\n",
        "word2vec_model = gensim.models.Word2Vec(target, size = dim, window = window, min_count = min_count)\n",
        "\n",
        "\n",
        "for token, i in TRG.vocab.stoi.items():\n",
        "  if token in word2vec_model.wv.vocab.keys():\n",
        "    word2vec_vectors.append(torch.FloatTensor(word2vec_model[token]))\n",
        "  else:\n",
        "    word2vec_vectors.append(torch.zeros(dim))\n",
        "TRG.vocab.set_vectors(TRG.vocab.stoi, word2vec_vectors, dim)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEgJBpw9aRIe",
        "outputId": "5c55d36c-a544-413c-ab83-dc6e32acfe2c"
      },
      "source": [
        "# Display the embeddings generated\n",
        "TRG.vocab.vectors"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUnTAQ6UGk8"
      },
      "source": [
        "word2vec_model.save('embeddings.txt')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16De39tX5k_x"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bDK-VwOU1VS"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 250):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_dBA9LvU8UN"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTiJSUOKU_k7"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIXulxZcVCLX"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ANHDYJ2VEUO"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 trainedEmbedding,\n",
        "                 max_length = 250):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.trainedEmbedding = trainedEmbedding\n",
        "        self.tok_embedding = nn.Embedding.from_pretrained(self.trainedEmbedding)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "            \n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8OPijdWVMfk"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        # query, key, value\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxCNguX0VP20"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAbFgSu16oKq"
      },
      "source": [
        "# Training Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXVYVXkTeU_5"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "trainedEmbedding = torch.FloatTensor(TRG.vocab.vectors)\n",
        "\n",
        "enc = Encoder(INPUT_DIM,\n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device, \n",
        "              trainedEmbedding)\n",
        "\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ih_mdaoVmfT"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim()>1 and not isinstance(m,nn.Embedding):\n",
        "        nn.init.xavier_normal_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights);"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JraEftvs6qeS",
        "outputId": "e3e31697-f56c-4faf-c839-7e23e3370572"
      },
      "source": [
        "def count_parameters(model):\n",
        "    total_params = 0\n",
        "    for p in model.parameters():\n",
        "        if p.requires_grad:\n",
        "            total_params+=p.numel()\n",
        "    print(f\"Model has {total_params:,} trainable parameters\")\n",
        "\n",
        "count_parameters(model)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model has 7,705,107 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mp34iAzVr1B"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "        if isinstance(criterion, nn.CTCLoss):\n",
        "            output_lengths = output.shape[1]\n",
        "            target_lengths = trg.shape[1]-1\n",
        "            output = output.contiguous().permute(1,0,2)\n",
        "            trg = trg[:,1:].contiguous()\n",
        "            loss = criterion(output, trg, output_lengths, target_lengths)\n",
        "        else:\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            loss = criterion(output, trg)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dofxS1LyVy7J"
      },
      "source": [
        "def evaluate(model, iterator, criterion, metrics=None):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            if isinstance(criterion, nn.CTCLoss):\n",
        "                output_lengths = torch.full(size=(output.shape[0],), fill_value = output.shape[1], dtype=torch.long)\n",
        "                target_lengths = trg.shape[1]-1\n",
        "                output = output.contiguous().permute(1,0,2)\n",
        "                trg = trg[:,1:].contiguous()\n",
        "                loss = criterion(output, trg, output_lengths, target_lengths)\n",
        "\n",
        "            else:\n",
        "                output = output.contiguous().view(-1, output_dim)\n",
        "                trg = trg[:,1:].contiguous().view(-1)\n",
        "                loss = criterion(output, trg)\n",
        "                \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEjwW6C1V2Db"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYkA4jf3VogT"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8CeuNllVqMH"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNcpdvFue5Ac",
        "outputId": "d0645190-f511-4c2c-d35b-d9dca9d26bcf"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 75\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 5s\n",
            "\tTrain Loss: 8.215 | Train PPL: 3694.560\n",
            "\t Val. Loss: 7.634 |  Val. PPL: 2066.789\n",
            "Epoch: 02 | Time: 0m 5s\n",
            "\tTrain Loss: 7.278 | Train PPL: 1448.787\n",
            "\t Val. Loss: 7.610 |  Val. PPL: 2017.391\n",
            "Epoch: 03 | Time: 0m 5s\n",
            "\tTrain Loss: 7.122 | Train PPL: 1239.321\n",
            "\t Val. Loss: 7.758 |  Val. PPL: 2340.751\n",
            "Epoch: 04 | Time: 0m 5s\n",
            "\tTrain Loss: 6.961 | Train PPL: 1054.944\n",
            "\t Val. Loss: 7.750 |  Val. PPL: 2322.053\n",
            "Epoch: 05 | Time: 0m 5s\n",
            "\tTrain Loss: 6.801 | Train PPL: 898.694\n",
            "\t Val. Loss: 7.696 |  Val. PPL: 2199.124\n",
            "Epoch: 06 | Time: 0m 5s\n",
            "\tTrain Loss: 6.709 | Train PPL: 819.516\n",
            "\t Val. Loss: 7.679 |  Val. PPL: 2162.342\n",
            "Epoch: 07 | Time: 0m 5s\n",
            "\tTrain Loss: 6.588 | Train PPL: 726.419\n",
            "\t Val. Loss: 7.669 |  Val. PPL: 2140.154\n",
            "Epoch: 08 | Time: 0m 5s\n",
            "\tTrain Loss: 6.496 | Train PPL: 662.247\n",
            "\t Val. Loss: 7.572 |  Val. PPL: 1943.175\n",
            "Epoch: 09 | Time: 0m 5s\n",
            "\tTrain Loss: 6.372 | Train PPL: 585.301\n",
            "\t Val. Loss: 7.548 |  Val. PPL: 1896.993\n",
            "Epoch: 10 | Time: 0m 5s\n",
            "\tTrain Loss: 6.273 | Train PPL: 530.009\n",
            "\t Val. Loss: 7.560 |  Val. PPL: 1919.774\n",
            "Epoch: 11 | Time: 0m 5s\n",
            "\tTrain Loss: 6.165 | Train PPL: 475.928\n",
            "\t Val. Loss: 7.450 |  Val. PPL: 1719.071\n",
            "Epoch: 12 | Time: 0m 5s\n",
            "\tTrain Loss: 6.040 | Train PPL: 419.877\n",
            "\t Val. Loss: 7.322 |  Val. PPL: 1512.821\n",
            "Epoch: 13 | Time: 0m 5s\n",
            "\tTrain Loss: 5.894 | Train PPL: 362.979\n",
            "\t Val. Loss: 7.273 |  Val. PPL: 1440.601\n",
            "Epoch: 14 | Time: 0m 5s\n",
            "\tTrain Loss: 5.778 | Train PPL: 323.167\n",
            "\t Val. Loss: 7.179 |  Val. PPL: 1311.909\n",
            "Epoch: 15 | Time: 0m 5s\n",
            "\tTrain Loss: 5.627 | Train PPL: 277.867\n",
            "\t Val. Loss: 7.134 |  Val. PPL: 1254.128\n",
            "Epoch: 16 | Time: 0m 5s\n",
            "\tTrain Loss: 5.473 | Train PPL: 238.076\n",
            "\t Val. Loss: 7.002 |  Val. PPL: 1098.761\n",
            "Epoch: 17 | Time: 0m 5s\n",
            "\tTrain Loss: 5.299 | Train PPL: 200.182\n",
            "\t Val. Loss: 6.928 |  Val. PPL: 1020.867\n",
            "Epoch: 18 | Time: 0m 5s\n",
            "\tTrain Loss: 5.141 | Train PPL: 170.956\n",
            "\t Val. Loss: 6.868 |  Val. PPL: 961.054\n",
            "Epoch: 19 | Time: 0m 5s\n",
            "\tTrain Loss: 4.987 | Train PPL: 146.547\n",
            "\t Val. Loss: 6.738 |  Val. PPL: 843.788\n",
            "Epoch: 20 | Time: 0m 5s\n",
            "\tTrain Loss: 4.811 | Train PPL: 122.829\n",
            "\t Val. Loss: 6.646 |  Val. PPL: 769.528\n",
            "Epoch: 21 | Time: 0m 5s\n",
            "\tTrain Loss: 4.665 | Train PPL: 106.168\n",
            "\t Val. Loss: 6.593 |  Val. PPL: 729.802\n",
            "Epoch: 22 | Time: 0m 5s\n",
            "\tTrain Loss: 4.500 | Train PPL:  90.060\n",
            "\t Val. Loss: 6.523 |  Val. PPL: 680.730\n",
            "Epoch: 23 | Time: 0m 5s\n",
            "\tTrain Loss: 4.320 | Train PPL:  75.196\n",
            "\t Val. Loss: 6.494 |  Val. PPL: 661.192\n",
            "Epoch: 24 | Time: 0m 5s\n",
            "\tTrain Loss: 4.165 | Train PPL:  64.399\n",
            "\t Val. Loss: 6.467 |  Val. PPL: 643.285\n",
            "Epoch: 25 | Time: 0m 5s\n",
            "\tTrain Loss: 4.003 | Train PPL:  54.745\n",
            "\t Val. Loss: 6.371 |  Val. PPL: 584.490\n",
            "Epoch: 26 | Time: 0m 5s\n",
            "\tTrain Loss: 3.821 | Train PPL:  45.631\n",
            "\t Val. Loss: 6.275 |  Val. PPL: 530.891\n",
            "Epoch: 27 | Time: 0m 5s\n",
            "\tTrain Loss: 3.693 | Train PPL:  40.156\n",
            "\t Val. Loss: 6.250 |  Val. PPL: 518.132\n",
            "Epoch: 28 | Time: 0m 5s\n",
            "\tTrain Loss: 3.532 | Train PPL:  34.208\n",
            "\t Val. Loss: 6.189 |  Val. PPL: 487.123\n",
            "Epoch: 29 | Time: 0m 5s\n",
            "\tTrain Loss: 3.384 | Train PPL:  29.493\n",
            "\t Val. Loss: 6.087 |  Val. PPL: 440.297\n",
            "Epoch: 30 | Time: 0m 5s\n",
            "\tTrain Loss: 3.250 | Train PPL:  25.792\n",
            "\t Val. Loss: 6.120 |  Val. PPL: 454.664\n",
            "Epoch: 31 | Time: 0m 5s\n",
            "\tTrain Loss: 3.105 | Train PPL:  22.318\n",
            "\t Val. Loss: 6.069 |  Val. PPL: 432.078\n",
            "Epoch: 32 | Time: 0m 5s\n",
            "\tTrain Loss: 2.976 | Train PPL:  19.614\n",
            "\t Val. Loss: 6.053 |  Val. PPL: 425.535\n",
            "Epoch: 33 | Time: 0m 5s\n",
            "\tTrain Loss: 2.826 | Train PPL:  16.882\n",
            "\t Val. Loss: 5.972 |  Val. PPL: 392.418\n",
            "Epoch: 34 | Time: 0m 5s\n",
            "\tTrain Loss: 2.687 | Train PPL:  14.684\n",
            "\t Val. Loss: 6.000 |  Val. PPL: 403.511\n",
            "Epoch: 35 | Time: 0m 5s\n",
            "\tTrain Loss: 2.568 | Train PPL:  13.042\n",
            "\t Val. Loss: 5.938 |  Val. PPL: 379.257\n",
            "Epoch: 36 | Time: 0m 5s\n",
            "\tTrain Loss: 2.465 | Train PPL:  11.759\n",
            "\t Val. Loss: 5.901 |  Val. PPL: 365.300\n",
            "Epoch: 37 | Time: 0m 5s\n",
            "\tTrain Loss: 2.336 | Train PPL:  10.339\n",
            "\t Val. Loss: 5.947 |  Val. PPL: 382.692\n",
            "Epoch: 38 | Time: 0m 5s\n",
            "\tTrain Loss: 2.258 | Train PPL:   9.562\n",
            "\t Val. Loss: 5.891 |  Val. PPL: 361.792\n",
            "Epoch: 39 | Time: 0m 5s\n",
            "\tTrain Loss: 2.139 | Train PPL:   8.495\n",
            "\t Val. Loss: 5.901 |  Val. PPL: 365.439\n",
            "Epoch: 40 | Time: 0m 5s\n",
            "\tTrain Loss: 2.020 | Train PPL:   7.536\n",
            "\t Val. Loss: 5.880 |  Val. PPL: 357.667\n",
            "Epoch: 41 | Time: 0m 5s\n",
            "\tTrain Loss: 1.922 | Train PPL:   6.835\n",
            "\t Val. Loss: 5.861 |  Val. PPL: 350.953\n",
            "Epoch: 42 | Time: 0m 5s\n",
            "\tTrain Loss: 1.803 | Train PPL:   6.065\n",
            "\t Val. Loss: 5.855 |  Val. PPL: 349.059\n",
            "Epoch: 43 | Time: 0m 5s\n",
            "\tTrain Loss: 1.729 | Train PPL:   5.636\n",
            "\t Val. Loss: 5.833 |  Val. PPL: 341.518\n",
            "Epoch: 44 | Time: 0m 5s\n",
            "\tTrain Loss: 1.635 | Train PPL:   5.128\n",
            "\t Val. Loss: 5.872 |  Val. PPL: 354.800\n",
            "Epoch: 45 | Time: 0m 5s\n",
            "\tTrain Loss: 1.564 | Train PPL:   4.777\n",
            "\t Val. Loss: 5.844 |  Val. PPL: 345.311\n",
            "Epoch: 46 | Time: 0m 5s\n",
            "\tTrain Loss: 1.503 | Train PPL:   4.496\n",
            "\t Val. Loss: 5.935 |  Val. PPL: 377.914\n",
            "Epoch: 47 | Time: 0m 5s\n",
            "\tTrain Loss: 1.421 | Train PPL:   4.142\n",
            "\t Val. Loss: 5.891 |  Val. PPL: 361.606\n",
            "Epoch: 48 | Time: 0m 5s\n",
            "\tTrain Loss: 1.336 | Train PPL:   3.802\n",
            "\t Val. Loss: 5.858 |  Val. PPL: 349.970\n",
            "Epoch: 49 | Time: 0m 5s\n",
            "\tTrain Loss: 1.269 | Train PPL:   3.558\n",
            "\t Val. Loss: 5.880 |  Val. PPL: 357.667\n",
            "Epoch: 50 | Time: 0m 5s\n",
            "\tTrain Loss: 1.212 | Train PPL:   3.361\n",
            "\t Val. Loss: 5.917 |  Val. PPL: 371.323\n",
            "Epoch: 51 | Time: 0m 5s\n",
            "\tTrain Loss: 1.150 | Train PPL:   3.158\n",
            "\t Val. Loss: 5.927 |  Val. PPL: 374.991\n",
            "Epoch: 52 | Time: 0m 5s\n",
            "\tTrain Loss: 1.095 | Train PPL:   2.990\n",
            "\t Val. Loss: 5.976 |  Val. PPL: 393.933\n",
            "Epoch: 53 | Time: 0m 5s\n",
            "\tTrain Loss: 1.041 | Train PPL:   2.832\n",
            "\t Val. Loss: 5.919 |  Val. PPL: 372.001\n",
            "Epoch: 54 | Time: 0m 5s\n",
            "\tTrain Loss: 0.984 | Train PPL:   2.676\n",
            "\t Val. Loss: 5.973 |  Val. PPL: 392.546\n",
            "Epoch: 55 | Time: 0m 5s\n",
            "\tTrain Loss: 0.957 | Train PPL:   2.604\n",
            "\t Val. Loss: 6.063 |  Val. PPL: 429.472\n",
            "Epoch: 56 | Time: 0m 5s\n",
            "\tTrain Loss: 0.900 | Train PPL:   2.460\n",
            "\t Val. Loss: 6.049 |  Val. PPL: 423.777\n",
            "Epoch: 57 | Time: 0m 5s\n",
            "\tTrain Loss: 0.844 | Train PPL:   2.326\n",
            "\t Val. Loss: 6.077 |  Val. PPL: 435.752\n",
            "Epoch: 58 | Time: 0m 5s\n",
            "\tTrain Loss: 0.789 | Train PPL:   2.200\n",
            "\t Val. Loss: 6.074 |  Val. PPL: 434.563\n",
            "Epoch: 59 | Time: 0m 5s\n",
            "\tTrain Loss: 0.771 | Train PPL:   2.162\n",
            "\t Val. Loss: 6.105 |  Val. PPL: 447.887\n",
            "Epoch: 60 | Time: 0m 5s\n",
            "\tTrain Loss: 0.738 | Train PPL:   2.091\n",
            "\t Val. Loss: 6.135 |  Val. PPL: 461.633\n",
            "Epoch: 61 | Time: 0m 5s\n",
            "\tTrain Loss: 0.683 | Train PPL:   1.980\n",
            "\t Val. Loss: 6.152 |  Val. PPL: 469.677\n",
            "Epoch: 62 | Time: 0m 5s\n",
            "\tTrain Loss: 0.672 | Train PPL:   1.959\n",
            "\t Val. Loss: 6.197 |  Val. PPL: 491.338\n",
            "Epoch: 63 | Time: 0m 5s\n",
            "\tTrain Loss: 0.653 | Train PPL:   1.921\n",
            "\t Val. Loss: 6.232 |  Val. PPL: 508.765\n",
            "Epoch: 64 | Time: 0m 5s\n",
            "\tTrain Loss: 0.614 | Train PPL:   1.848\n",
            "\t Val. Loss: 6.241 |  Val. PPL: 513.165\n",
            "Epoch: 65 | Time: 0m 5s\n",
            "\tTrain Loss: 0.579 | Train PPL:   1.785\n",
            "\t Val. Loss: 6.267 |  Val. PPL: 526.745\n",
            "Epoch: 66 | Time: 0m 5s\n",
            "\tTrain Loss: 0.552 | Train PPL:   1.737\n",
            "\t Val. Loss: 6.277 |  Val. PPL: 532.142\n",
            "Epoch: 67 | Time: 0m 5s\n",
            "\tTrain Loss: 0.531 | Train PPL:   1.701\n",
            "\t Val. Loss: 6.358 |  Val. PPL: 577.350\n",
            "Epoch: 68 | Time: 0m 5s\n",
            "\tTrain Loss: 0.510 | Train PPL:   1.666\n",
            "\t Val. Loss: 6.362 |  Val. PPL: 579.353\n",
            "Epoch: 69 | Time: 0m 5s\n",
            "\tTrain Loss: 0.501 | Train PPL:   1.650\n",
            "\t Val. Loss: 6.356 |  Val. PPL: 576.182\n",
            "Epoch: 70 | Time: 0m 5s\n",
            "\tTrain Loss: 0.456 | Train PPL:   1.577\n",
            "\t Val. Loss: 6.403 |  Val. PPL: 603.639\n",
            "Epoch: 71 | Time: 0m 5s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.552\n",
            "\t Val. Loss: 6.414 |  Val. PPL: 610.408\n",
            "Epoch: 72 | Time: 0m 5s\n",
            "\tTrain Loss: 0.433 | Train PPL:   1.541\n",
            "\t Val. Loss: 6.455 |  Val. PPL: 636.187\n",
            "Epoch: 73 | Time: 0m 5s\n",
            "\tTrain Loss: 0.423 | Train PPL:   1.526\n",
            "\t Val. Loss: 6.459 |  Val. PPL: 638.725\n",
            "Epoch: 74 | Time: 0m 5s\n",
            "\tTrain Loss: 0.397 | Train PPL:   1.488\n",
            "\t Val. Loss: 6.560 |  Val. PPL: 706.068\n",
            "Epoch: 75 | Time: 0m 5s\n",
            "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
            "\t Val. Loss: 6.504 |  Val. PPL: 667.747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R29QPxZo7aSm"
      },
      "source": [
        "# Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZD6TOXIwPnl"
      },
      "source": [
        "import spacy\n",
        "def text2Python(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "    \n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('en')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyHtkIIl7wQq"
      },
      "source": [
        "# Sample Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xba9pFjxwSUi",
        "outputId": "eb1dfba2-4800-4b17-ab2c-5af5dee3b20b"
      },
      "source": [
        "sourceText = 'write a python program to prints common letters in two input strings\\n '\n",
        "\n",
        "print(sourceText )\n",
        "\n",
        "translation, attention = text2Python(sourceText , SRC, TRG, model, device)\n",
        "\n",
        "for i in range(len(translation)):\n",
        "  print(translation[i])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "write a python program to prints common letters in two input strings\n",
            " \n",
            "s1='python'\n",
            "s2='schoolofai'\n",
            "a=list(set(s1)&set(s2))\n",
            "print(\"the common letters are:\")\n",
            "for i in a:\n",
            "    print(i)\n",
            "\n",
            "end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xTzK-8r8TXu"
      },
      "source": [
        "# Testing the code on 25 randomly generated samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZDfXeC_wTjD",
        "outputId": "d276e026-d655-424c-9230-cd98dbcff689"
      },
      "source": [
        "import random\n",
        "\n",
        "for i in range(25):\n",
        "  inf = random.randint(1,2500)\n",
        "  src = vars(train_data.examples[inf])['src']\n",
        "\n",
        "  print(\"\\n\\nQuestion: \", ' '.join(src));\n",
        "  print(\"\\n\")\n",
        "  \n",
        "  translation, attention = text2Python(src , SRC, TRG, model, device)\n",
        "\n",
        "  for i in range(len(translation)):\n",
        "    print(translation[i])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Question:    100 write a python program that checks if a string is a pallindrome\n",
            "\n",
            "\n",
            "\n",
            "def is_palindrome(st):\n",
            "    st = st.lower()\n",
            "    rev_st = st[::-1]\n",
            "    try:\n",
            "        assert rev_st == st\n",
            "        return true\n",
            "    except assertionerror:\n",
            "        return false\n",
            "\n",
            "st = \"nitin\"\n",
            "print(is_palindrome(st))\n",
            "\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a python function to generate random number between 2 integers\n",
            "\n",
            "\n",
            "def random_number(a, b):\n",
            "    import random\n",
            "    return random.randint(a, b)\n",
            "\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a python class to print all possible subsets from a set of distinct integers\n",
            "\n",
            "\n",
            "class sub:\n",
            "    def f1(self, s1):\n",
            "        return self.f2([], sorted(s1))\n",
            "    def f2(self, curr, s1):\n",
            "        if s1:\n",
            "            return self.f2(curr, s1[1:]) + self.f2(curr + [s1[0]], s1[1:])\n",
            "        return [curr]\n",
            "a=[2, 3, 5, 6, 4, 5]\n",
            "print(\"subsets: \")\n",
            "print(sub().f1(a))\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a python function to get prominent words from user test corpus using tfidf vectorizer\n",
            "\n",
            "\n",
            "def get_words(corpus, new_doc, top=2):\n",
            "    import numpy as np\n",
            "    from sklearn.feature_extraction.text import tfidfvectorizer\n",
            "    tfidf = tfidfvectorizer(stop_words='english')\n",
            "    if not corpus:\n",
            "        corpus = [\n",
            "            'i would like to check this document',\n",
            "            'how about one more document',\n",
            "            'aim is to capture the key words from the corpus',\n",
            "            'frequency of words in a document is called term frequency'\n",
            "        ]\n",
            "    x = tfidf.fit_transform(corpus)\n",
            "    feature_names = np.array(tfidf.get_feature_names())\n",
            "    if not new_doc:\n",
            "        new_doc = ['can key words in this new document be identified?',\n",
            "                   'idf is the inverse document frequency calculated for each of the words']\n",
            "    responses = tfidf.transform(new_doc)\n",
            "    def get_top_tf_idf_words(response, top_n=top):\n",
            "        sorted_nzs = np.argsort(response.data)[:-(top_n + 1):-1]\n",
            "        return feature_names[response.indices[sorted_nzs]]\n",
            "    print([get_top_tf_idf_words(response, 2) for response in responses])\n",
            "\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a python function to count number of lists in a list of lists\n",
            "\n",
            "\n",
            "def countlist(lst):\n",
            "    count = 0\n",
            "    for el in lst:\n",
            "        if type(el)== type([]):\n",
            "            count+= 1\n",
            "    return count\n",
            "\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a python function to return fermat 's sequence\n",
            "\n",
            "\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    a scalene triangle is a triangle that has three unequal sides .\n",
            "\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:  42 write a function to subtract two matrices in python\n",
            "\n",
            "\n",
            "matrix1 = [[0, 1, 2],\n",
            "           [3, 5, 5],\n",
            "           [6, 7, 8]]\n",
            "matrix2 = [[1, 2, 3],\n",
            "           [4, 5, 6],\n",
            "           [7, 8, 9]]\n",
            "def subtractthematrix(matrix1, matrix2):\n",
            "    matrix1rows = len(matrix1)\n",
            "    matrix2rows = len(matrix2)\n",
            "    matrix1col = len(matrix1[0])\n",
            "    matrix2col = len(matrix2[0])\n",
            "    \n",
            "end\n",
            "\n",
            "\n",
            "Question:    unique_justseen('abbccad ' , str.lower ) -- > a b c a d\n",
            "\n",
            "\n",
            "    return map(next, map(operator.itemgetter(1), groupby(iterable, key)))\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a python function to return minimum sum of factors of a number\n",
            "\n",
            "\n",
            "def findminsum(num):\n",
            "    sum = 0\n",
            "    i = 2\n",
            "    while(i * i <= num):\n",
            "        while(num % i == 0):\n",
            "            sum += i\n",
            "            num /= i\n",
            "        i += 1\n",
            "    sum += num\n",
            "    return sum\n",
            "\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:  write a function to compress a given string . suppose a character ' c ' occurs consecutively x times in the string . replace these consecutive occurrences of the character ' c ' with   ( x , c ) in the string .\n",
            "\n",
            "\n",
            "def compress(text):\n",
            "    from itertools import groupby\n",
            "    for k, g in groupby(text):\n",
            "        print(\"({}, {})\".format(len(list(g)), k), end=\" \")\n",
            "\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a boolean python function to check if a given string matches a given pattern\n",
            "\n",
            "\n",
            "import re\n",
            "def match(pattern, string):\n",
            "    if re.match(pattern, string):\n",
            "        return true\n",
            "    return false\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a python function to remove falsy values from a list\n",
            "\n",
            "\n",
            "def newlist(lst):\n",
            "  return list(filter(none, lst))\n",
            "\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a function to return the time taken by a given of moving object based of distance travelled in given time\n",
            "\n",
            "\n",
            "def cal_time(distance:float,speed:float)->float:\n",
            "    return distance/speed\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a python program to convert binary to gray code\n",
            "\n",
            "\n",
            "def gray_to_binary(n):\n",
            "    \"\"\"convert gray codeword to binary and return it.\"\"\"\n",
            "    n = int(n, 2)\n",
            "    n ^= (n >> 1)\n",
            "    return bin(n)[2:]\n",
            "        mask >>= 1\n",
            "        n ^= mask\n",
            "    return bin(n)[2:]\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a function to calculate and return electricity bill . units used are given . price per unit is fixed and is increased after 750 units .\n",
            "\n",
            "\n",
            "def calc_elect_bill(units):\n",
            "    if units > 0:\n",
            "        if units <= 750:\n",
            "            return 5*units\n",
            "        else:\n",
            "            return 5*(750) + 7*(units-750)\n",
            "    else:\n",
            "        return -1\n",
            "\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write python program to create a dictionary with key as first character and value as words starting with that character\n",
            "\n",
            "\n",
            "string_input = '''geeksforgeeks is a computer science portal for geeks.\n",
            "    it contains well written, well thought and well explained\n",
            "    computer science and programming articles, quizzes etc.'''\n",
            "words = string_input.split()\n",
            "dictionary = {}\n",
            "for word in words:\n",
            "    if (word[0].lower() not in dictionary.keys()):\n",
            "        dictionary[word[0].lower()] = []\n",
            "        dictionary[word[0].lower()].append(word)\n",
            "    else:\n",
            "        if (word not in dictionary[word[0].lower()]):\n",
            "            dictionary[word[0].lower()].append(word)\n",
            "print(dictionary)\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a python program to delete an element from a list\n",
            "\n",
            "\n",
            "list = ['a', 'bc', 'd', 'e']\n",
            "element = 'bc'\n",
            "list.remove(element)\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    nÃ¢Â‹ Â… 2n Ã¢ÂˆÂ’ 1 , with n Ã¢Â‰Â¥ 1 .\n",
            "\n",
            "\n",
            "def woodall_number(n):\n",
            "    if n >= 0:\n",
            "        return n * 2 ** n - 1\n",
            "\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a python program that takes height in centimeters as user input and return height in feet and inches\n",
            "\n",
            "\n",
            "cm=int(input(\"enter the height in centimeters:\"))\n",
            "inches=0.394*cm\n",
            "feet=0.0328*cm\n",
            "print(\"the length in inches\",round(inches,2))\n",
            "print(\"the length in feet\",round(feet,2))\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a python function that capitalize the first letter of every word in the file\n",
            "\n",
            "\n",
            "def capitalize(fname):\n",
            "    with open(fname, 'r') as f:\n",
            "        for line in f:\n",
            "            l=line.title()\n",
            "            print(l)\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:  60 write a function program to reverse the digits of an integer .\n",
            "\n",
            "\n",
            "def reverse_integer(x):\n",
            "        sign = -1 if x < 0 else 1\n",
            "        x *= sign\n",
            "        \n",
            "end\n",
            "\n",
            "\n",
            "Question:    to convert from a 1-d to 3-d\n",
            "\n",
            "\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a python program to typecast an integer to string and print it\n",
            "\n",
            "\n",
            "x = 2\n",
            "y = str(x)\n",
            "print(y)\n",
            "\n",
            "end\n",
            "\n",
            "\n",
            "Question:    write a python program to print the following floating numbers with no decimal places .\n",
            "\n",
            "\n",
            "x = 3.1415926\n",
            "y = -12.9999\n",
            "print(\"\\noriginal number: \", x)\n",
            "print(\"formatted number with no decimal places: \"+\"{:.0f}\".format(x))\n",
            "print(\"original number: \", y)\n",
            "print(\"formatted number with no decimal places: \"+\"{:.0f}\".format(y))\n",
            "\n",
            "\n",
            "end\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}